{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import accelerate\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + \"/../\")\n",
    "\n",
    "from src.utils.constants import CHEMBL_DATA_FILE, MODELS_DIR, LLAMA_3P3_70B_MODEL_DIR, LLAMA_3P3_70B_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82507/82507 [01:36<00:00, 858.40it/s] \n"
     ]
    }
   ],
   "source": [
    "with h5py.File(CHEMBL_DATA_FILE, 'r') as f:\n",
    "    paper_ids = list(f.keys())\n",
    "\n",
    "    abstracts = []\n",
    "    canon_SMILES_lists = []\n",
    "    for paper_name in tqdm(paper_ids):\n",
    "        h5_dataset = f[paper_name]\n",
    "        abstract = h5_dataset['abstract'][()].decode('utf-8')\n",
    "        abstracts.append(abstract)\n",
    "        compounds_list = json.loads(h5_dataset['compounds'][()].decode('utf-8'))\n",
    "        canon_SMILES = [compound['canonical_smiles'] for compound in compounds_list]\n",
    "        canon_SMILES_lists.append(canon_SMILES)\n",
    "\n",
    "data = list(zip(canon_SMILES_lists, abstracts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['COc1ccc2[nH]c3ccc4cc[n+](CCO)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCO)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCO)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCO)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](CCN4CCCCC4)ccc3ccc1n2C.[Cl-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](CCN4CCCCC4)ccc3ccc1n2C.[Cl-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](CCN4CCCCC4)ccc3ccc1n2C.[Cl-]',\n",
       "  'COc1ccc2c(c1)c1c3ccncc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ccncc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ccncc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ccncc3ccc1n2C',\n",
       "  'C[n+]1ccc2c(ccc3[nH]c4ccc(O)cc4c32)c1.[I-]',\n",
       "  'C[n+]1ccc2c(ccc3[nH]c4ccc(O)cc4c32)c1.[I-]',\n",
       "  'C[n+]1ccc2c(ccc3[nH]c4ccc(O)cc4c32)c1.[I-]',\n",
       "  'Oc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](C)ccc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](C)ccc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](C)ccc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3c[n+](C)ccc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCN5CCCCC5)cc4c3c2c1.[Cl-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCN5CCCCC5)cc4c3c2c1.[Cl-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](CCN5CCCCC5)cc4c3c2c1.[Cl-]',\n",
       "  'COc1ccc2[nH]c3ccc4c[n+](C)ccc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4c[n+](C)ccc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4c[n+](C)ccc4c3c2c1.[I-]',\n",
       "  'C[n+]1cccc2c3c(ccc21)[nH]c1ccccc13.[I-]',\n",
       "  'C[n+]1cccc2c3c(ccc21)[nH]c1ccccc13.[I-]',\n",
       "  'C[n+]1cccc2c3c(ccc21)[nH]c1ccccc13.[I-]',\n",
       "  'Oc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](C)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](C)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cc[n+](C)cc4c3c2c1.[I-]',\n",
       "  'C[n+]1ccc2ccc3[nH]c4ccc(O)cc4c3c2c1.[I-]',\n",
       "  'C[n+]1ccc2ccc3[nH]c4ccc(O)cc4c3c2c1.[I-]',\n",
       "  'C[n+]1ccc2ccc3[nH]c4ccc(O)cc4c3c2c1.[I-]',\n",
       "  'COc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ncccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ncccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ncccc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ncccc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'Oc1ccc2[nH]c3ccc4cnccc4c3c2c1',\n",
       "  'COc1ccc2c(c1)c1c3ncccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ncccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ncccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3ncccc3ccc1n2C',\n",
       "  'COc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4cccnc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'COc1ccc2[nH]c3ccc4ccncc4c3c2c1',\n",
       "  'COc1ccc2c(c1)c1c3cc[n+](C)cc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3cc[n+](C)cc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3cc[n+](C)cc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3cc[n+](C)cc3ccc1n2C.[I-]',\n",
       "  'COc1ccc2c(c1)c1c3cnccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3cnccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3cnccc3ccc1n2C',\n",
       "  'COc1ccc2c(c1)c1c3cnccc3ccc1n2C'],\n",
       " 'The DNA intercalating compounds derived from 6H-pyridocarbazole (ellipticines, olivacines) elicit high antitumor properties. In order to get information about the mechanism of action of these agents it is necessary to study structurally related analogues. For this purpose, various derivatives of the four isomeric 7H-pyridocarbazoles were synthesized by a single photochemical process on indolylpyridylethylenes. These derivatives are able to intercalate into DNA. The DNA binding affinities vary in the range of 10(4) to 10(6) M-1, depending mainly on the nature of the substituent, nitrogen quaternization being the most enhancing factor. The position of the pyridinic nitrogen does not markedly affect the DNA binding affinity. Three quaternized compounds elicit a significative but low antileukemic activity on L1210 mice leukemia. The properties of 7H-pyridocarbazoles are discussed and compared to those of 6H-pyridocarbazoles (ellipticines and olivacines).')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, cache_dir):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer from the Hugging Face Hub. If the model is not\n",
    "    found, it will be downloaded and cached in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The name of the model to load.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the model is already cached\n",
    "    if os.path.exists(cache_dir):\n",
    "        print(f\"Model {model_name} already cached in {cache_dir}.\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            cache_dir,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cache_dir,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(f\"Downloading model {model_name} to {cache_dir}...\")\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokenizer, texts):\n",
    "    \"\"\"\n",
    "    Get the embeddings for a given list of strings using the specified model and tokenizer.\n",
    "    Args:\n",
    "        texts (list): A list of strings to encode.\n",
    "        model: The model to use for generating embeddings.\n",
    "        tokenizer: The tokenizer to use for encoding the text.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The embeddings for the input text.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move the inputs to the same device as the model\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Forward pass to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Get the mean of the last hidden state across the sequence length (do not include padding)\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    mask_expanded = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(embeddings * mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model meta-llama/Llama-3.3-70B-Instruct already cached in /data2/scratch/junhalee/extract-llama-embed/models/models--meta-llama--Llama-3.3-70B-Instruct/snapshots/6f6073b423013f6a7d4d9f39144961bfbfbc386b/.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff3462b702148ee825ed06faed5ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = LLAMA_3P3_70B_MODEL_NAME\n",
    "cache_dir = LLAMA_3P3_70B_MODEL_DIR\n",
    "\n",
    "model, tokenizer = load_model(model_name, cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embed_tokens.weight, Device: cuda:1\n",
      "Layer: layers.0.self_attn.q_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.self_attn.k_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.self_attn.v_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.self_attn.o_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.mlp.gate_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.mlp.up_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.mlp.down_proj.weight, Device: cuda:1\n",
      "Layer: layers.0.input_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.0.post_attention_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.1.self_attn.q_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.self_attn.k_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.self_attn.v_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.self_attn.o_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.mlp.gate_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.mlp.up_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.mlp.down_proj.weight, Device: cuda:1\n",
      "Layer: layers.1.input_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.1.post_attention_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.2.self_attn.q_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.self_attn.k_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.self_attn.v_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.self_attn.o_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.mlp.gate_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.mlp.up_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.mlp.down_proj.weight, Device: cuda:1\n",
      "Layer: layers.2.input_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.2.post_attention_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.3.self_attn.q_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.self_attn.k_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.self_attn.v_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.self_attn.o_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.mlp.gate_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.mlp.up_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.mlp.down_proj.weight, Device: cuda:1\n",
      "Layer: layers.3.input_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.3.post_attention_layernorm.weight, Device: cuda:1\n",
      "Layer: layers.4.self_attn.q_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.self_attn.k_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.self_attn.v_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.self_attn.o_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.mlp.gate_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.mlp.up_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.mlp.down_proj.weight, Device: cuda:2\n",
      "Layer: layers.4.input_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.4.post_attention_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.5.self_attn.q_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.self_attn.k_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.self_attn.v_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.self_attn.o_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.mlp.gate_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.mlp.up_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.mlp.down_proj.weight, Device: cuda:2\n",
      "Layer: layers.5.input_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.5.post_attention_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.6.self_attn.q_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.self_attn.k_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.self_attn.v_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.self_attn.o_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.mlp.gate_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.mlp.up_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.mlp.down_proj.weight, Device: cuda:2\n",
      "Layer: layers.6.input_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.6.post_attention_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.7.self_attn.q_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.self_attn.k_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.self_attn.v_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.self_attn.o_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.mlp.gate_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.mlp.up_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.mlp.down_proj.weight, Device: cuda:2\n",
      "Layer: layers.7.input_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.7.post_attention_layernorm.weight, Device: cuda:2\n",
      "Layer: layers.8.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.8.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.8.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.9.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.9.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.9.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.10.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.10.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.10.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.11.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.11.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.11.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.12.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.12.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.12.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.13.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.13.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.13.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.14.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.14.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.14.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.15.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.15.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.15.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.16.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.16.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.16.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.17.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.17.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.17.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.18.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.18.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.18.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.19.self_attn.q_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.self_attn.k_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.self_attn.v_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.self_attn.o_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.mlp.gate_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.mlp.up_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.mlp.down_proj.weight, Device: cuda:4\n",
      "Layer: layers.19.input_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.19.post_attention_layernorm.weight, Device: cuda:4\n",
      "Layer: layers.20.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.20.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.20.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.21.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.21.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.21.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.22.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.22.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.22.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.23.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.23.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.23.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.24.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.24.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.24.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.25.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.25.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.25.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.26.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.26.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.26.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.27.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.27.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.27.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.28.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.28.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.28.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.29.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.29.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.29.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.30.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.30.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.30.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.31.self_attn.q_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.self_attn.k_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.self_attn.v_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.self_attn.o_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.mlp.gate_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.mlp.up_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.mlp.down_proj.weight, Device: cuda:6\n",
      "Layer: layers.31.input_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.31.post_attention_layernorm.weight, Device: cuda:6\n",
      "Layer: layers.32.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.32.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.32.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.32.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.32.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.32.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.32.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.32.input_layernorm.weight, Device: meta\n",
      "Layer: layers.32.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.33.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.33.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.33.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.33.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.33.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.33.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.33.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.33.input_layernorm.weight, Device: meta\n",
      "Layer: layers.33.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.34.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.34.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.34.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.34.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.34.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.34.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.34.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.34.input_layernorm.weight, Device: meta\n",
      "Layer: layers.34.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.35.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.35.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.35.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.35.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.35.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.35.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.35.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.35.input_layernorm.weight, Device: meta\n",
      "Layer: layers.35.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.36.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.36.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.36.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.36.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.36.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.36.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.36.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.36.input_layernorm.weight, Device: meta\n",
      "Layer: layers.36.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.37.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.37.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.37.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.37.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.37.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.37.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.37.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.37.input_layernorm.weight, Device: meta\n",
      "Layer: layers.37.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.38.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.38.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.38.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.38.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.38.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.38.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.38.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.38.input_layernorm.weight, Device: meta\n",
      "Layer: layers.38.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.39.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.39.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.39.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.39.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.39.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.39.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.39.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.39.input_layernorm.weight, Device: meta\n",
      "Layer: layers.39.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.40.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.40.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.40.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.40.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.40.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.40.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.40.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.40.input_layernorm.weight, Device: meta\n",
      "Layer: layers.40.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.41.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.41.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.41.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.41.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.41.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.41.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.41.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.41.input_layernorm.weight, Device: meta\n",
      "Layer: layers.41.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.42.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.42.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.42.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.42.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.42.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.42.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.42.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.42.input_layernorm.weight, Device: meta\n",
      "Layer: layers.42.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.43.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.43.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.43.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.43.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.43.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.43.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.43.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.43.input_layernorm.weight, Device: meta\n",
      "Layer: layers.43.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.44.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.44.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.44.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.44.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.44.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.44.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.44.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.44.input_layernorm.weight, Device: meta\n",
      "Layer: layers.44.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.45.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.45.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.45.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.45.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.45.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.45.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.45.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.45.input_layernorm.weight, Device: meta\n",
      "Layer: layers.45.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.46.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.46.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.46.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.46.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.46.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.46.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.46.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.46.input_layernorm.weight, Device: meta\n",
      "Layer: layers.46.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.47.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.47.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.47.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.47.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.47.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.47.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.47.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.47.input_layernorm.weight, Device: meta\n",
      "Layer: layers.47.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.48.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.48.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.48.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.48.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.48.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.48.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.48.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.48.input_layernorm.weight, Device: meta\n",
      "Layer: layers.48.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.49.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.49.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.49.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.49.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.49.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.49.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.49.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.49.input_layernorm.weight, Device: meta\n",
      "Layer: layers.49.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.50.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.50.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.50.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.50.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.50.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.50.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.50.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.50.input_layernorm.weight, Device: meta\n",
      "Layer: layers.50.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.51.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.51.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.51.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.51.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.51.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.51.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.51.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.51.input_layernorm.weight, Device: meta\n",
      "Layer: layers.51.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.52.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.52.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.52.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.52.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.52.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.52.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.52.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.52.input_layernorm.weight, Device: meta\n",
      "Layer: layers.52.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.53.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.53.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.53.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.53.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.53.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.53.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.53.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.53.input_layernorm.weight, Device: meta\n",
      "Layer: layers.53.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.54.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.54.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.54.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.54.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.54.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.54.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.54.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.54.input_layernorm.weight, Device: meta\n",
      "Layer: layers.54.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.55.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.55.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.55.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.55.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.55.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.55.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.55.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.55.input_layernorm.weight, Device: meta\n",
      "Layer: layers.55.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.56.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.56.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.56.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.56.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.56.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.56.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.56.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.56.input_layernorm.weight, Device: meta\n",
      "Layer: layers.56.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.57.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.57.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.57.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.57.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.57.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.57.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.57.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.57.input_layernorm.weight, Device: meta\n",
      "Layer: layers.57.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.58.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.58.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.58.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.58.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.58.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.58.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.58.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.58.input_layernorm.weight, Device: meta\n",
      "Layer: layers.58.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.59.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.59.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.59.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.59.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.59.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.59.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.59.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.59.input_layernorm.weight, Device: meta\n",
      "Layer: layers.59.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.60.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.60.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.60.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.60.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.60.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.60.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.60.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.60.input_layernorm.weight, Device: meta\n",
      "Layer: layers.60.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.61.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.61.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.61.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.61.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.61.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.61.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.61.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.61.input_layernorm.weight, Device: meta\n",
      "Layer: layers.61.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.62.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.62.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.62.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.62.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.62.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.62.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.62.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.62.input_layernorm.weight, Device: meta\n",
      "Layer: layers.62.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.63.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.63.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.63.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.63.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.63.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.63.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.63.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.63.input_layernorm.weight, Device: meta\n",
      "Layer: layers.63.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.64.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.64.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.64.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.64.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.64.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.64.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.64.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.64.input_layernorm.weight, Device: meta\n",
      "Layer: layers.64.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.65.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.65.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.65.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.65.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.65.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.65.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.65.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.65.input_layernorm.weight, Device: meta\n",
      "Layer: layers.65.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.66.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.66.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.66.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.66.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.66.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.66.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.66.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.66.input_layernorm.weight, Device: meta\n",
      "Layer: layers.66.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.67.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.67.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.67.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.67.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.67.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.67.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.67.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.67.input_layernorm.weight, Device: meta\n",
      "Layer: layers.67.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.68.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.68.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.68.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.68.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.68.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.68.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.68.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.68.input_layernorm.weight, Device: meta\n",
      "Layer: layers.68.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.69.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.69.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.69.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.69.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.69.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.69.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.69.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.69.input_layernorm.weight, Device: meta\n",
      "Layer: layers.69.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.70.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.70.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.70.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.70.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.70.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.70.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.70.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.70.input_layernorm.weight, Device: meta\n",
      "Layer: layers.70.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.71.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.71.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.71.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.71.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.71.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.71.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.71.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.71.input_layernorm.weight, Device: meta\n",
      "Layer: layers.71.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.72.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.72.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.72.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.72.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.72.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.72.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.72.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.72.input_layernorm.weight, Device: meta\n",
      "Layer: layers.72.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.73.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.73.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.73.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.73.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.73.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.73.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.73.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.73.input_layernorm.weight, Device: meta\n",
      "Layer: layers.73.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.74.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.74.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.74.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.74.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.74.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.74.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.74.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.74.input_layernorm.weight, Device: meta\n",
      "Layer: layers.74.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.75.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.75.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.75.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.75.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.75.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.75.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.75.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.75.input_layernorm.weight, Device: meta\n",
      "Layer: layers.75.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.76.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.76.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.76.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.76.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.76.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.76.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.76.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.76.input_layernorm.weight, Device: meta\n",
      "Layer: layers.76.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.77.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.77.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.77.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.77.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.77.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.77.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.77.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.77.input_layernorm.weight, Device: meta\n",
      "Layer: layers.77.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.78.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.78.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.78.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.78.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.78.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.78.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.78.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.78.input_layernorm.weight, Device: meta\n",
      "Layer: layers.78.post_attention_layernorm.weight, Device: meta\n",
      "Layer: layers.79.self_attn.q_proj.weight, Device: meta\n",
      "Layer: layers.79.self_attn.k_proj.weight, Device: meta\n",
      "Layer: layers.79.self_attn.v_proj.weight, Device: meta\n",
      "Layer: layers.79.self_attn.o_proj.weight, Device: meta\n",
      "Layer: layers.79.mlp.gate_proj.weight, Device: meta\n",
      "Layer: layers.79.mlp.up_proj.weight, Device: meta\n",
      "Layer: layers.79.mlp.down_proj.weight, Device: meta\n",
      "Layer: layers.79.input_layernorm.weight, Device: meta\n",
      "Layer: layers.79.post_attention_layernorm.weight, Device: meta\n",
      "Layer: norm.weight, Device: meta\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, Device: {param.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for the abstracts in batches \n",
    "batch_size = 32\n",
    "embeddings = []\n",
    "for i in tqdm(range(0, len(abstracts), batch_size)):\n",
    "    batch = abstracts[i:i + batch_size]\n",
    "    batch_embeddings = get_embeddings(model, tokenizer, batch)\n",
    "    embeddings.append(batch_embeddings)\n",
    "embeddings = torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the HDF5 file\n",
    "\"\"\"\n",
    "File has 82507 papers\n",
    "- Each paper is keyed by CHEMBL ID (ranges from CHEMBL1121361 to CHEMBL5483193), keying leads to a Group\n",
    "- Each paper is a group with 3 keys: 'abstract', 'compounds', and 'doi', keying leads to a Dataset (needs to be decoded to utf-8)\n",
    "    - 'abstract' is a string of the abstract\n",
    "    - 'doi' is a string of the doi\n",
    "    - 'compounds' is a string of the compounds, which are in SMILES format\n",
    "\n",
    "keys are the CHEMBL IDs ranging from 'CHEMBL1121361' to 'CHEMBL5483193'\n",
    "Key -> Group, each group again has \n",
    "\"\"\"\n",
    "\n",
    "with h5py.File(CHEMBL_DATA_FILE, 'r') as f:\n",
    "    dataset_name = list(f.keys())[2000]\n",
    "    dataset = f[dataset_name]\n",
    "\n",
    "    abstract = dataset['abstract'][()].decode('utf-8')\n",
    "    doi = dataset['doi'][()].decode('utf-8')\n",
    "    compounds = json.loads(dataset['compounds'][()].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(CHEMBL_DATA_FILE, 'r') as f:\n",
    "    dataset_names = list(f.keys())\n",
    "\n",
    "    # Get all the abstracts along with their corresponding CHEMBL IDs\n",
    "    abstracts = {}\n",
    "    for dataset_name in tqdm(dataset_names):\n",
    "        dataset = f[dataset_name]\n",
    "        abstract = dataset['abstract'][()].decode('utf-8')\n",
    "        abstracts[dataset_name] = abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(CHEMBL_DATA_FILE, 'r') as f:\n",
    "    paper_ids = list(f.keys())\n",
    "\n",
    "    abstracts = []\n",
    "    for paper_name in tqdm(paper_ids):\n",
    "        h5_dataset = f[paper_name]\n",
    "        abstract = h5_dataset['abstract'][()].decode('utf-8')\n",
    "        abstracts.append(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
